{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearningTechExamples.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s60XBfox2nwl"
      },
      "source": [
        "Kevin Casey "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whSUvadwg6XG"
      },
      "source": [
        "Required import of fashion classifier dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb_XTRnSpxKP"
      },
      "source": [
        "#Input the fashion image dataset in order to train the image classifier\n",
        "import tensorflow as tf \n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "#Split the input into training values and testing values\n",
        "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFSaJB3KguBa"
      },
      "source": [
        "Example given to replicate without using the keras library in the next code cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8xYiOJFp4RY"
      },
      "source": [
        "import os, datetime\n",
        "def create_model():\n",
        "  return tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "\n",
        "\n",
        "def train_model():\n",
        "  model = create_model()\n",
        "  model.summary()\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "  model.fit(x=x_train, \n",
        "            y=y_train, \n",
        "            epochs=50, \n",
        "            validation_data=(x_test, y_test),\n",
        "            callbacks=[tensorboard_callback])\n",
        "\n",
        "train_model()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHRikuU04u6J"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5k7rskxBwRQ"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oc-11jkEhXGi"
      },
      "source": [
        "Recreating the above image classifier without using the keras library\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7vX_MIgdpFB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6702ea5-8751-46c5-a3a5-4ac5bb0fed77"
      },
      "source": [
        "#Kevin Casey \n",
        "\n",
        "#the install may be required based on current version\n",
        "#!pip install tensorflow==2.0.0\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time as time\n",
        "\n",
        "batch_size = 128\n",
        "nb_epoch = 50\n",
        "nb_classes = 10\n",
        "\n",
        "#initialize the starting weights and biases\n",
        "w0 = tf.Variable(\n",
        "    tf.random.normal(\n",
        "    shape=[784, 512],\n",
        "    mean=0.0,\n",
        "    stddev=1.0,\n",
        "    seed=43,\n",
        "    dtype=tf.dtypes.float64),\n",
        "    trainable=True)\n",
        "b0 = tf.Variable(\n",
        "    tf.random.normal(\n",
        "    shape=[batch_size, 512],\n",
        "    mean=0.0,\n",
        "    stddev=1.0,\n",
        "    seed=43,\n",
        "    dtype=tf.dtypes.float64),\n",
        "    trainable=True)\n",
        "\n",
        "w1 = tf.Variable(\n",
        "    tf.random.normal(\n",
        "    shape=[512, 10],\n",
        "    mean=0.0,\n",
        "   stddev=1.0,\n",
        "    seed=43,\n",
        "    dtype=tf.dtypes.float64),\n",
        "    trainable=True)\n",
        "b1 = tf.Variable(\n",
        "    tf.random.normal(\n",
        "    shape=[batch_size, 10],\n",
        "    mean=0.0,\n",
        "    stddev=1.0,\n",
        "    seed=43,\n",
        "    dtype=tf.dtypes.float64),\n",
        "    trainable=True)\n",
        "\n",
        "\n",
        "\n",
        "def sm_dense(y, w, b):\n",
        "  return tf.nn.softmax(tf.matmul(y, w) + b)\n",
        "\n",
        "def relu_dense(y, w, b):\n",
        "  return tf.nn.relu(tf.matmul(y, w) + b)\n",
        "\n",
        "\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "#swap all data sets to tensors to work with\n",
        "x_train = tf.convert_to_tensor(x_train, dtype=tf.float64)\n",
        "x_test = tf.convert_to_tensor(x_test, dtype=tf.float64)\n",
        "y_train = tf.convert_to_tensor(y_train, dtype=tf.float64)\n",
        "y_test = tf.convert_to_tensor(y_test, dtype=tf.float64)\n",
        "\n",
        "#create the model\n",
        "def multilayer_perceptron(x, w0, b0, w1, b1):\n",
        "  x = tf.reshape(x, [-1, 784])\n",
        "  x = relu_dense(x, w0, b0)\n",
        "  tf.nn.dropout(x, 0.2)\n",
        "  x = sm_dense(x, w1, b1)\n",
        "  return x\n",
        "\n",
        "#I set the learning rate relatively high as it was running into a substantial\n",
        "#amount of local minima, getting stuck quickly at 0.1\n",
        "learning_rate = 0.5\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for epoch in range(nb_epoch):\n",
        "  epoch_time = time.time()\n",
        "  #create the shuffled batches here and iterate through them\n",
        "  for batch_iter in range(x_train.shape[0]//128):\n",
        "    #iterates through the epoch until there are no more training values\n",
        "    offset = batch_iter * batch_size\n",
        "    batch_data = x_train[offset:(offset + batch_size), :] \n",
        "    batch_labels = y_train[offset:(offset + batch_size):]\n",
        "\n",
        "    #creates the model and determines the loss to obtain the gradients\n",
        "    with tf.GradientTape() as tape:\n",
        "      tape.watch([w0, w1, b0, b1])\n",
        "      model = multilayer_perceptron(batch_data, w0, b0, w1, b1)\n",
        "      batch_labels = tf.cast(batch_labels, tf.int64)\n",
        "      loss = tf.nn.sparse_softmax_cross_entropy_with_logits( \n",
        "                          labels=batch_labels, logits=model)\n",
        "      cost = tf.reduce_mean(loss)    \n",
        "\n",
        "    gradsw0, gradsw1, gradsb0, gradsb1 = tape.gradient(cost, [w0,w1,b0,b1])\n",
        "\n",
        "    #the gradient descent steps for each set of weights and biases.\n",
        "    #the training level is significantly slower than the keras implementation,\n",
        "    #yet it still learns until a local minima is hit, feedback on why the \n",
        "    #training went as slow as it did would be appreciated\n",
        "    w0 = tf.Variable(w0 - gradsw0*learning_rate*cost)\n",
        "    w1 = tf.Variable(w1 - gradsw1*learning_rate*cost)\n",
        "    b0 = tf.Variable(b0 - gradsb0*learning_rate*cost)\n",
        "    b1 = tf.Variable(b1 - gradsb1*learning_rate*cost)\n",
        "\n",
        "  print('iter:', epoch, 'loss:', cost.numpy(), \"Epoch time: \", time.time() - epoch_time)\n",
        "\n",
        "#total training time\n",
        "print(\"Training time: \", time.time() - start)\n",
        "\n",
        "#using the test values to test how well the training worked on a new set of data\n",
        "for batch_iter in range(x_test.shape[0]//128):\n",
        "  offset = batch_iter * batch_size\n",
        "  batch_data = x_test[offset:(offset + batch_size), :] \n",
        "  batch_labels = y_test[offset:(offset + batch_size):]\n",
        "  with tf.GradientTape() as tape:\n",
        "    tape.watch([w0, w1, b0, b1])\n",
        "    model = multilayer_perceptron(batch_data, w0, b0, w1, b1)\n",
        "    batch_labels = tf.cast(batch_labels, tf.int64)\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits( \n",
        "                            labels=batch_labels, logits=model)\n",
        "    cost = tf.reduce_mean(loss)    \n",
        "\n",
        "print(\"Verify loss: \", cost.numpy())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter: 0 loss: 2.156429134379184 Epoch time:  2.5958216190338135\n",
            "iter: 1 loss: 2.155854388329394 Epoch time:  2.428013801574707\n",
            "iter: 2 loss: 2.1482726634416114 Epoch time:  2.5278735160827637\n",
            "iter: 3 loss: 2.1486398851850335 Epoch time:  2.431572198867798\n",
            "iter: 4 loss: 2.1480707813668616 Epoch time:  2.4691028594970703\n",
            "iter: 5 loss: 2.060290979679099 Epoch time:  2.453275442123413\n",
            "iter: 6 loss: 2.05455721835415 Epoch time:  2.5925471782684326\n",
            "iter: 7 loss: 1.9926709408548242 Epoch time:  2.5873537063598633\n",
            "iter: 8 loss: 1.9759071909067454 Epoch time:  2.652812957763672\n",
            "iter: 9 loss: 1.9610042312289282 Epoch time:  2.629941701889038\n",
            "iter: 10 loss: 1.9609136760582064 Epoch time:  2.4646148681640625\n",
            "iter: 11 loss: 1.96087225811852 Epoch time:  2.4449496269226074\n",
            "iter: 12 loss: 1.9767786799889877 Epoch time:  2.4581403732299805\n",
            "iter: 13 loss: 1.960850530435548 Epoch time:  2.4461209774017334\n",
            "iter: 14 loss: 1.9767007027960417 Epoch time:  2.456088066101074\n",
            "iter: 15 loss: 1.9608361386371471 Epoch time:  2.4493794441223145\n",
            "iter: 16 loss: 1.9610735407338615 Epoch time:  2.4761738777160645\n",
            "iter: 17 loss: 1.961065459339075 Epoch time:  2.4461803436279297\n",
            "iter: 18 loss: 1.9608839218087701 Epoch time:  2.437439441680908\n",
            "iter: 19 loss: 1.9608166352005596 Epoch time:  2.443556785583496\n",
            "iter: 20 loss: 1.9609272165094354 Epoch time:  2.4442553520202637\n",
            "iter: 21 loss: 1.9610898769559522 Epoch time:  2.4249088764190674\n",
            "iter: 22 loss: 1.9607951150454854 Epoch time:  2.4507689476013184\n",
            "iter: 23 loss: 1.9606266587689873 Epoch time:  2.4276437759399414\n",
            "iter: 24 loss: 1.961143564549476 Epoch time:  2.464844226837158\n",
            "iter: 25 loss: 1.961146242361164 Epoch time:  2.4461584091186523\n",
            "iter: 26 loss: 1.9683657795762617 Epoch time:  2.4450955390930176\n",
            "iter: 27 loss: 1.8750971121334394 Epoch time:  2.425666093826294\n",
            "iter: 28 loss: 1.8673999028521 Epoch time:  2.4577159881591797\n",
            "iter: 29 loss: 1.8774422521525236 Epoch time:  2.444087266921997\n",
            "iter: 30 loss: 1.8829701096436926 Epoch time:  2.4196016788482666\n",
            "iter: 31 loss: 1.8751618315993102 Epoch time:  2.4340851306915283\n",
            "iter: 32 loss: 1.8749948168133894 Epoch time:  2.4182584285736084\n",
            "iter: 33 loss: 1.8749893370673072 Epoch time:  2.4591636657714844\n",
            "iter: 34 loss: 1.874937979445351 Epoch time:  2.455577850341797\n",
            "iter: 35 loss: 1.8749930868061568 Epoch time:  2.450711488723755\n",
            "iter: 36 loss: 1.8746722189108502 Epoch time:  2.45593523979187\n",
            "iter: 37 loss: 1.874824003490462 Epoch time:  2.443458318710327\n",
            "iter: 38 loss: 1.8747421789987468 Epoch time:  2.419961929321289\n",
            "iter: 39 loss: 1.8752102794629937 Epoch time:  2.4452033042907715\n",
            "iter: 40 loss: 1.8751968833214634 Epoch time:  2.4381463527679443\n",
            "iter: 41 loss: 1.875209968092769 Epoch time:  2.481257200241089\n",
            "iter: 42 loss: 1.8748623239867652 Epoch time:  2.4419147968292236\n",
            "iter: 43 loss: 1.8751810586297337 Epoch time:  2.4522931575775146\n",
            "iter: 44 loss: 1.8748618631164515 Epoch time:  2.444448232650757\n",
            "iter: 45 loss: 1.8748535562778095 Epoch time:  2.451512098312378\n",
            "iter: 46 loss: 1.8750030414026178 Epoch time:  2.4339184761047363\n",
            "iter: 47 loss: 1.874851458374185 Epoch time:  2.416891574859619\n",
            "iter: 48 loss: 1.874940410111532 Epoch time:  2.43721866607666\n",
            "iter: 49 loss: 1.874511787777625 Epoch time:  2.4281880855560303\n",
            "Training time:  123.17096543312073\n",
            "Verify loss:  1.9451985453881866\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5Lr5Mld9JS_"
      },
      "source": [
        "Comparing and optimizing using different hardware run times for calculating matrix multiplication "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqclY5Xo9TVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5eb4cd4-0300-4e6f-84ca-1cedc503e1a0"
      },
      "source": [
        "#Kevin Casey 11214791 kjc887\n",
        "\n",
        "#the install is required and a runtime restart is sometimes required as well\n",
        "\n",
        "#!pip install tensorflow-gpu==2.0.0\n",
        "from tabulate import tabulate as table\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time as time\n",
        "import sys as sys\n",
        "\n",
        "print(tf.config.experimental.list_physical_devices())\n",
        "\n",
        "matrix_sizes = [500, 1000, 5000, 10000]\n",
        "cpu_times = []\n",
        "gpu_times = []\n",
        "\n",
        "#first iteration through matrix_sizes runs on CPU\n",
        "with tf.device('/device:CPU:0'):\n",
        "  for size in matrix_sizes:\n",
        "    #randomly generated matrices to multiply\n",
        "    matrixA = tf.random.uniform(shape = [size,size],\n",
        "                                  minval=0,\n",
        "                                  maxval=None,\n",
        "                                  dtype=tf.dtypes.float32,\n",
        "                                  seed=1337,\n",
        "                                  name='a')\n",
        "\n",
        "    matrixB = tf.random.uniform(shape = [size,size],\n",
        "                                  minval=0,\n",
        "                                  maxval=None,\n",
        "                                  dtype=tf.dtypes.float32,\n",
        "                                  seed=1337,\n",
        "                                  name='b')\n",
        "\n",
        "    #starts time right before multiplication runs, ends time after function call and adds to to the CPU time list\n",
        "    start_time = time.time()\n",
        "\n",
        "    multcpu = tf.matmul(matrixA, matrixB)\n",
        "\n",
        "    cpu_times.append(time.time() - start_time)\n",
        "\n",
        "\n",
        "#this will run the exact same as the cpu, besides the device it is initially run on and the list to add the times to\n",
        "with tf.device('/device:GPU:0'):\n",
        "  for size in matrix_sizes:\n",
        "    matrixA = tf.random.uniform(shape = [size, size],\n",
        "                                  minval=0,\n",
        "                                  maxval=None,\n",
        "                                  dtype=tf.dtypes.float32,\n",
        "                                  seed=1337,\n",
        "                                  name='a')\n",
        "\n",
        "    matrixB = tf.random.uniform(shape = [size, size],\n",
        "                                  minval=0,\n",
        "                                  maxval=None,\n",
        "                                  dtype=tf.dtypes.float32,\n",
        "                                  seed=1337,\n",
        "                                  name='b')\n",
        "\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    multgpu = tf.matmul(matrixA, matrixB)\n",
        "\n",
        "    gpu_times.append(time.time() - start_time)\n",
        "\n",
        "#prints off the table with all data and headers\n",
        "print(table([[\"CPU\" , cpu_times[0], cpu_times[1], cpu_times[2], cpu_times[3]], \n",
        "             [\"GPU\", gpu_times[0], gpu_times[1], gpu_times[2], gpu_times[3]]], headers=['Device', '500', '1000', '5000', '10000'],\n",
        "             tablefmt='orgtbl'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "| Device   |         500 |        1000 |       5000 |      10000 |\n",
            "|----------+-------------+-------------+------------+------------|\n",
            "| CPU      | 0.0497587   | 0.0304668   | 3.57925    | 27.4216    |\n",
            "| GPU      | 0.000656843 | 0.000158787 | 6.8903e-05 |  0.0132332 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqM7gy2I9FgY"
      },
      "source": [
        "Finding Jacobian values using 2x2 matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ9VCtxv9Gvg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "5210ebd8-db0d-4c4f-9dd0-bc7261f61d93"
      },
      "source": [
        "#!pip install tensorflow==2.0.0\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def func(x):\n",
        "  return (3. * tf.pow(x,2.) + 2.*x + 3)\n",
        "\n",
        "\n",
        "x1 = tf.Variable([1., 3.], trainable=True)\n",
        "x2 = tf.Variable([1., 3.], trainable=True)\n",
        "learning_rate = 0.1\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y1 = func(x1)\n",
        "jacob = tape.jacobian(y1, x1)\n",
        "print(jacob)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y2 = func(x2)\n",
        "jacob = tape.jacobian(y2, x2)\n",
        "print(jacob)\n",
        "\n",
        "#both jacobians combined will work out to [[8, 20], [8, 20]] which results in a\n",
        "#jacobian value of 0 (finding the determinent of the jacobian matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 8.  0.]\n",
            " [ 0. 20.]], shape=(2, 2), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 8.  0.]\n",
            " [ 0. 20.]], shape=(2, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}